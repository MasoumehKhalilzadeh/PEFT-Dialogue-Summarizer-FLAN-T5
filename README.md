# PEFT Dialogue Summarizer FLAN-T5
A project using PEFT (LoRA) to fine-tune FLAN-T5 for multi-turn dialogue summarization on the SAMSum dataset.


# ğŸ§  Dialogue Summarizer with FLAN-T5 + PEFT (LoRA)

This project shows how to use **Parameter Efficient Fine-Tuning (PEFT)** with **LoRA adapters** to fine-tune the `FLAN-T5` model for **multi-turn dialogue summarization** using the [SAMSum dataset](https://huggingface.co/datasets/samsum).


---

## ğŸ§© Whatâ€™s Inside

- ğŸ¤– Model: `google/flan-t5-base`
- ğŸ“š Dataset: `samsum` (chat dialogue + summaries)
- âš¡ Fine-Tuning Method: **PEFT (LoRA)**
- ğŸ› ï¸ Framework: Hugging Face Transformers
- ğŸš€ Runtime: Google Colab 


---

## ğŸ” Sample Results

**ğŸ—¨ï¸ Input Dialogue:**

Mom: Are you coming for dinner?
Me: I can't, I'm still at work.
Mom: OK, Iâ€™ll save you some food.
Me: Thanks, love you!


**âœ… Real Summary:**
> Mom asks about dinner; child replies they're working late.

**ğŸ¤– Model Summary (PEFT):**
> Person is working late and Mom will save dinner for them.

---


## ğŸ“ˆ What I Learned

- How to fine-tune large language models **efficiently**
- How **LoRA** reduces training cost while keeping good performance
- How to use Hugging Faceâ€™s `transformers`, `datasets`, and `peft` libraries together

---

## ğŸ“š Dataset Info

- **SAMSum Dataset**  
  > 16,000+ short chat conversations with human-written summaries  
  ğŸ“¦ [View on Hugging Face](https://huggingface.co/datasets/samsum)

---

## ğŸ”§ Requirements

Install needed libraries:
```bash
pip install transformers datasets peft accelerate evaluate sentencepiece py7zr

