
# ğŸ§  Dialogue Summarization with FLAN-T5 + PEFT (LoRA)

This project demonstrates how to efficiently fine-tune a large language model (LLM), `FLAN-T5-base`, using **Parameter-Efficient Fine-Tuning (PEFT)** with **LoRA adapters**, for the task of dialogue summarization. We use the **SAMSum dataset**, which contains multi-turn dialogues and human-written summaries. 

With just **1,000 training samples** the model achieves **strong ROUGE scores**, showing that PEFT can provide high-quality performance with minimal compute and data.

---

## ğŸ” Project Highlights

- âœ… Fine-tuned `google/flan-t5-base` using **LoRA** (via Hugging Face PEFT)
- âœ… Trained on **1,000 samples** from the SAMSum dataset
- âœ… Evaluated on 300 test samples with **100 used for ROUGE scoring**
- âœ… Achieved **ROUGE-L: 0.31**, **ROUGE-1: 0.36** â€” strong results!
- âœ… Runs in under **20 minutes** on Colab Pro (A100 GPU)

---

## ğŸ“¦ Tech Stack

| Tool          | Purpose                             |
|---------------|-------------------------------------|
| ğŸ¤— Transformers | Load and fine-tune FLAN-T5 model     |
| ğŸ¤— Datasets     | Access and preprocess SAMSum dataset |
| ğŸ¤— PEFT         | Apply LoRA adapters for efficient tuning |
| ğŸ¤— Evaluate     | Compute ROUGE metrics               |
| ğŸ Python       | Core scripting language              |
| ğŸ§  Google Colab | Environment to train LLMs  |

---

## ğŸ“š Dataset: SAMSum

- **Name**: [SAMSum](https://huggingface.co/datasets/samsum)
- **Samples**: ~15,000 human chat dialogues + summaries
- **Train used**: First 1,000 samples
- **Test used**: First 300 samples
- **Task**: Summarize informal dialogues in English

---

## âš™ï¸ LoRA Configuration

```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q", "v"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM
)




